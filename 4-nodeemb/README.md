### NodeEmb概述

NodeEmb是一种用于图表示学习（Graph Representation Learning）的方法，主要目的是将图中的节点映射到低维向量空间中，使得图的结构信息能够通过这些向量表示出来。这种表示可以用于多种下游任务，如节点分类、链路预测、图分类、异常节点检测和聚类等。

### 1. 输入图与特征工程

1. **输入图（Input Graph）**：
    - 图的顶点集合 $ V $
    - 二值化的邻接矩阵 $ A $

2. **特征工程（Feature Engineering）**：
    - 传统方法：手动提取节点级、边级和图级特征，然后通过机器学习算法（如SVM、神经网络等）将这些特征映射到标签。
    - 表示学习：自动化特征学习，减少手动特征工程的需求。

### 2. 节点嵌入目标

节点嵌入的目标是将图中的节点映射到一个低维向量空间中，使得节点在嵌入空间中的相似度能够近似反映图中的相似度。例如，连接边的两个节点在嵌入空间中应靠得较近。具体目标可以表述为：优化嵌入，使得节点对的内积近似于它们在图中的相似度。

### 3. 节点相似性度量与嵌入优化

1. **相似性度量**：
    - 基于节点的连接
    - 共享邻居
    - 相似结构角色

2. **编码器与解码器（Encoder and Decoder）框架**：
    - **编码器（Encoder）**：将每个节点映射到低维向量空间中，常见的方法有DeepWalk和node2vec。
    - **解码器（Decoder）**：定义节点相似度，并将嵌入映射到相似度得分上，优化编码器参数使得嵌入空间的相似度接近图中节点的相似度。

### 4. 随机游走与负采样

1. **随机游走（Random Walk）**：
    - 从每个节点出发进行固定长度的随机游走，生成节点的邻域。
    - 随机游走的策略（如DeepWalk和node2vec）可以是无偏或有偏的，有偏的随机游走可以调节局部和全局视角。

2. **负采样（Negative Sampling）**：
    - 为了加速优化过程，只对部分随机采样的负节点进行归一化计算。
    - 使用噪声对比估计（Noise Contrastive Estimation）方法来近似最大化softmax的对数概率。

### 5. 梯度下降与优化

- **梯度下降（Gradient Descent）**：初始化节点嵌入向量，通过计算和更新梯度来最小化损失函数。
- **随机梯度下降（Stochastic Gradient Descent, SGD）**：每次只对一个节点进行更新，以提高计算效率。

### 6. 嵌入的使用

嵌入向量可以用于多种任务：
- **节点分类**：预测节点的标签。
- **链路预测**：预测两个节点之间是否存在边。
- **图分类**：通过聚合节点嵌入来表示整个图。

### 7. 常见方法

- **DeepWalk**：基于无偏随机游走的节点嵌入方法。
- **node2vec**：基于有偏随机游走的节点嵌入方法，通过调节游走的返回参数和离开参数来平衡局部和全局视角。

### 8. 限制与未来方向

当前方法的限制包括：
- **传导性方法（Transductive Method）**：不能对训练集中不存在的节点进行嵌入计算，不能应用于新图或动态变化的图。
- **无法捕捉结构相似性**：如DeepWalk和node2vec难以捕捉具有相同结构角色的节点相似性。


### 4. 随机游走与负采样

**随机游走（Random Walk）** 和 **负采样（Negative Sampling）** 是图嵌入学习中常用的方法。下面我们详细展开这两个方法的原理及其具体应用。

#### 4.1 随机游走（Random Walk）

**随机游走** 是一种从一个节点开始，根据一定的概率规则遍历图的方法。其目的是生成节点序列，这些序列可以用作训练深度学习模型的输入，从而学习节点的嵌入表示。

##### 4.1.1 随机游走策略

1. **无偏随机游走（Unbiased Random Walk）**：
    - 无偏随机游走是最简单的随机游走方法，具体步骤如下：
      1. 从一个初始节点开始。
      2. 在每一步，均匀随机选择一个邻居节点并移动到该节点。
      3. 重复上述步骤，直到达到预定的游走长度。
    - 这种方法会生成均匀分布的节点序列，适合捕捉节点的局部结构。

2. **有偏随机游走（Biased Random Walk）**：
    - 有偏随机游走通过引入偏差参数来调节游走的方向性，从而捕捉图的全局结构。常用的方法有Node2Vec。
    - Node2Vec中的有偏随机游走引入了两个参数：返回参数 $ p $ 和离开参数 $ q $。
    - **Node2Vec随机游走算法**：
        1. 初始节点为 $ u $，当前节点为 $ v $，上一个节点为 $ t $。
        2. 计算当前节点 $ v $ 的邻居节点 $ x $ 的转移概率 $ \pi(v,x) $：
            - 如果 $ x = t $（返回上一个节点），则 $ \pi(v,x) = 1/p $。
            - 如果 $ x $ 是 $ v $ 的邻居且 $ x \neq t $，则 $ \pi(v,x) = 1 $。
            - 如果 $ x $ 不是 $ v $ 的邻居，则 $ \pi(v,x) = 1/q $。
        3. 根据转移概率选择下一个节点并移动。
        4. 重复上述步骤，直到达到预定的游走长度。

#### 4.2 负采样（Negative Sampling）

**负采样** 是一种有效的优化技术，用于训练深度学习模型，特别是在大规模数据集上的模型。其主要思想是通过对部分随机选择的负样本进行优化，而不是对所有样本进行优化，从而加速模型的训练过程。

##### 4.2.1 负采样原理

1. **背景**：
    - 在嵌入学习中，目标是最大化正确节点对的相似度，同时最小化错误节点对的相似度。
    - 传统方法需要对每个节点对进行计算，这在大规模图上是不可行的。

2. **负采样方法**：
    - 负采样通过对部分随机选择的负样本进行归一化计算，来近似最大化softmax的对数概率。
    - **具体步骤**：
        1. 对每个正样本（即真实存在的节点对）采样多个负样本（即不存在的节点对）。
        2. 计算正样本的相似度，并最小化其负对数似然。
        3. 计算负样本的相似度，并最大化其负对数似然。

3. **公式表示**：
    - 对于正样本对 $(u,v)$，目标是最大化：
      $$
      \log \sigma(f(u)^\top f(v))
      $$
    - 对于负样本对 $(u,n)$，目标是最小化：
      $$
      \sum_{n \in N(u)} \log \sigma(-f(u)^\top f(n))
      $$
    - 其中，$\sigma$ 是sigmoid函数，$N(u)$ 是从背景分布中采样的负样本集合。

4. **噪声对比估计（Noise Contrastive Estimation, NCE）**：
    - NCE是一种近似方法，通过对数概率的最大化来区分正样本和负样本。
    - 使用NCE时，负样本的选择应尽可能多样化，以更好地训练模型。

##### 4.2.2 Node2Vec中的负采样

在Node2Vec中，负采样的具体实现如下：

1. **采样负样本**：从背景分布中为每个正样本采样 $ k $ 个负样本。
2. **计算损失函数**：结合正样本和负样本计算损失函数：
   $$
   L = - \log \sigma(f(u)^\top f(v)) - \sum_{n \in N(u)} \log \sigma(-f(u)^\top f(n))
   $$
3. **梯度下降优化**：使用随机梯度下降法（SGD）对嵌入向量进行优化，更新节点的嵌入表示。

通过随机游走生成节点序列，然后使用负采样加速优化过程，Node2Vec能够有效地学习图中节点的嵌入表示。这种方法不仅可以捕捉局部结构，还能平衡局部与全局信息，为图的各种下游任务提供高质量的节点表示。

以上是对随机游走与负采样的详细解释。如果有具体的应用或实现示例需求，请告诉我。